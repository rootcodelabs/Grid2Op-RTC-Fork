{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid2Op integration with ray / rllib framework\n",
    "\n",
    "Try me out interactively with: [![Binder](./img/badge_logo.svg)](https://mybinder.org/v2/gh/rte-france/Grid2Op/master)\n",
    "\n",
    "\n",
    "**objectives** This notebooks briefly explains how to use grid2op with ray (rllib) RL framework. Make sure to read the previous notebook 11_IntegrationWithExistingRLFrameworks.ipynb for a deeper dive into what happens. We only show the working solution here.\n",
    "\n",
    "<font color='red'> This explains the ideas and shows a \"self contained\" somewhat minimal example of use of ray / rllib framework with grid2op. It is not meant to be fully generic, code might need to be adjusted.</font> \n",
    "\n",
    "This notebook is more an \"example of what works\" rather than a deep dive tutorial.\n",
    "\n",
    "See https://docs.ray.io/en/latest/rllib/rllib-env.html#configuring-environments for a more detailed information.\n",
    "\n",
    "See also https://docs.ray.io/en/latest/rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.html for other details\n",
    "\n",
    "This notebook is tested with grid2op 1.10 and ray 2.23 on an ubuntu 20.04 machine.\n",
    "\n",
    "\n",
    "## 1 Create the \"Grid2opEnv\" class\n",
    "\n",
    "In the next cell, we define a custom environment (that will internally use the `GymEnv` grid2op class) that is needed for ray / rllib.\n",
    "\n",
    "Indeed, in order to work with ray / rllib you need to define a custom wrapper on top of the GymEnv wrapper. You then have:\n",
    "\n",
    "- self._g2op_env which is the default grid2op environment, receiving grid2op Action and producing grid2op Observation.\n",
    "- self._gym_env which is a the grid2op defined `gymnasium Environment` that cannot be directly used with ray / rllib\n",
    "- `Grid2opEnv` which is a the wrapper on top of `self._gym_env` to make it usable with ray / rllib.\n",
    "\n",
    "Ray / rllib expects the gymnasium environment to inherit from `gymnasium.Env` and to be initialized with a given configuration. This is why you need to create the `Grid2opEnv` wrapper on top of `GymEnv`.\n",
    "\n",
    "In the initialization of `Grid2opEnv`, the `env_config` variable is a dictionary that can take as key-word arguments:\n",
    "\n",
    "- `backend_cls` : what is the class of the backend. If not provided, it will use `LightSimBackend` from the `lightsim2grid` package\n",
    "- `backend_options`: what options will be used to create the backend for your environment. Your backend will be created by calling\n",
    "   `backend_cls(**backend_options)`, for example if you want to build `LightSimBackend(detailed_info_for_cascading_failure=False)` you can pass `{\"backend_cls\": LightSimBackend, \"backend_options\": {\"detailed_info_for_cascading_failure\": False}}`\n",
    "- `env_name` : name of the grid2op environment you want to use, by default it uses `\"l2rpn_case14_sandbox\"`\n",
    "- `env_is_test` : whether to add `test=True` when creating the grid2op environment (if `env_is_test` is True it will add `test=True` when calling `grid2op.make(..., test=True)`) otherwise it uses `test=False`\n",
    "- `obs_attr_to_keep` : in this wrapper we only allow your agent to see a Box as an observation. This parameter allows you to control which attributes of the grid2op observation will be present in the agent observation space. By default it's `[\"rho\", \"p_or\", \"gen_p\", \"load_p\"]` which is \"kind of random\" and is probably not suited for every agent.\n",
    "- `act_type` : controls the type of actions your agent will be able to perform. Already coded in this notebook are:\n",
    "   - `\"discrete\"` to use a `Discrete` action space\n",
    "   - `\"box\"` to use a `Box` action space\n",
    "   - `\"multi_discrete\"` to use a `MultiDiscrete` action space\n",
    "- `act_attr_to_keep` :  that allows you to customize the action space. If not provided, it defaults to:\n",
    "  - `[\"set_line_status_simple\", \"set_bus\"]` if `act_type` is `\"discrete\"` \n",
    "  - `[\"redispatch\", \"set_storage\", \"curtail\"]` if `act_type` is `\"box\"` \n",
    "  - `[\"one_line_set\", \"one_sub_set\"]` if `act_type` is `\"multi_discrete\"`\n",
    "\n",
    "If you want to add more customization, for example the reward function, the parameters of the environment etc. etc. feel free to get inspired by this code and extend it. Any PR on this regard is more than welcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Discrete, MultiDiscrete, Box\n",
    "\n",
    "import ray\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.algorithms import ppo\n",
    "\n",
    "from typing import Dict, Literal, Any\n",
    "\n",
    "import grid2op\n",
    "from grid2op.gym_compat import GymEnv, BoxGymObsSpace, DiscreteActSpace, BoxGymActSpace, MultiDiscreteActSpace\n",
    "from lightsim2grid import LightSimBackend\n",
    "\n",
    "\n",
    "class Grid2opEnv(Env):\n",
    "    def __init__(self,\n",
    "                 env_config: Dict[Literal[\"backend_cls\",\n",
    "                                          \"backend_options\",\n",
    "                                          \"env_name\",\n",
    "                                          \"env_is_test\",\n",
    "                                          \"obs_attr_to_keep\",\n",
    "                                          \"act_type\",\n",
    "                                          \"act_attr_to_keep\"],\n",
    "                                  Any]):\n",
    "        super().__init__()\n",
    "        if env_config is None:\n",
    "            env_config = {}\n",
    "\n",
    "        # handle the backend\n",
    "        backend_cls = LightSimBackend\n",
    "        if \"backend_cls\" in env_config:\n",
    "            backend_cls = env_config[\"backend_cls\"]\n",
    "        backend_options = {}\n",
    "        if \"backend_options\" in env_config:\n",
    "            backend_options = env_config[\"backend_options\"]\n",
    "        backend = backend_cls(**backend_options)\n",
    "\n",
    "        # create the grid2op environment\n",
    "        env_name = \"l2rpn_case14_sandbox\"\n",
    "        if \"env_name\" in env_config:\n",
    "            env_name = env_config[\"env_name\"]\n",
    "        if \"env_is_test\" in env_config:\n",
    "            is_test = bool(env_config[\"env_is_test\"])\n",
    "        else:\n",
    "            is_test = False\n",
    "        self._g2op_env = grid2op.make(env_name, backend=backend, test=is_test)\n",
    "        # NB by default this might be really slow (when the environment is reset)\n",
    "        # see https://grid2op.readthedocs.io/en/latest/data_pipeline.html for maybe 10x speed ups !\n",
    "        # TODO customize reward or action_class for example !\n",
    "\n",
    "        # create the gym env (from grid2op)\n",
    "        self._gym_env = GymEnv(self._g2op_env)\n",
    "\n",
    "        # customize observation space\n",
    "        obs_attr_to_keep = [\"rho\", \"p_or\", \"gen_p\", \"load_p\"]\n",
    "        if \"obs_attr_to_keep\" in env_config:\n",
    "            obs_attr_to_keep = copy.deepcopy(env_config[\"obs_attr_to_keep\"])\n",
    "        self._gym_env.observation_space.close()\n",
    "        self._gym_env.observation_space = BoxGymObsSpace(self._g2op_env.observation_space,\n",
    "                                                         attr_to_keep=obs_attr_to_keep\n",
    "                                                         )\n",
    "        # export observation space for the Grid2opEnv\n",
    "        self.observation_space = Box(shape=self._gym_env.observation_space.shape,\n",
    "                                     low=self._gym_env.observation_space.low,\n",
    "                                     high=self._gym_env.observation_space.high)\n",
    "\n",
    "        # customize the action space\n",
    "        act_type = \"discrete\"\n",
    "        if \"act_type\" in env_config:\n",
    "            act_type = env_config[\"act_type\"]\n",
    "\n",
    "        self._gym_env.action_space.close()\n",
    "        if act_type == \"discrete\":\n",
    "            # user wants a discrete action space\n",
    "            act_attr_to_keep =  [\"set_line_status_simple\", \"set_bus\"]\n",
    "            if \"act_attr_to_keep\" in env_config:\n",
    "                act_attr_to_keep = copy.deepcopy(env_config[\"act_attr_to_keep\"])\n",
    "            self._gym_env.action_space = DiscreteActSpace(self._g2op_env.action_space,\n",
    "                                                          attr_to_keep=act_attr_to_keep)\n",
    "            self.action_space = Discrete(self._gym_env.action_space.n)\n",
    "        elif act_type == \"box\":\n",
    "            # user wants continuous action space\n",
    "            act_attr_to_keep =  [\"redispatch\", \"set_storage\", \"curtail\"]\n",
    "            if \"act_attr_to_keep\" in env_config:\n",
    "                act_attr_to_keep = copy.deepcopy(env_config[\"act_attr_to_keep\"])\n",
    "            self._gym_env.action_space = BoxGymActSpace(self._g2op_env.action_space,\n",
    "                                                        attr_to_keep=act_attr_to_keep)\n",
    "            self.action_space = Box(shape=self._gym_env.action_space.shape,\n",
    "                                    low=self._gym_env.action_space.low,\n",
    "                                    high=self._gym_env.action_space.high)\n",
    "        elif act_type == \"multi_discrete\":\n",
    "            # user wants a multi-discrete action space\n",
    "            act_attr_to_keep = [\"one_line_set\", \"one_sub_set\"]\n",
    "            if \"act_attr_to_keep\" in env_config:\n",
    "                act_attr_to_keep = copy.deepcopy(env_config[\"act_attr_to_keep\"])\n",
    "            self._gym_env.action_space = MultiDiscreteActSpace(self._g2op_env.action_space,\n",
    "                                                               attr_to_keep=act_attr_to_keep)\n",
    "            self.action_space = MultiDiscrete(self._gym_env.action_space.nvec)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"action type '{act_type}' is not currently supported.\")\n",
    "            \n",
    "            \n",
    "    def reset(self, seed, options):\n",
    "        # use default _gym_env (from grid2op.gym_compat module)\n",
    "        return self._gym_env.reset(seed=seed, options=options)\n",
    "        \n",
    "    def step(self, action):\n",
    "        # use default _gym_env (from grid2op.gym_compat module)\n",
    "        return self._gym_env.step(action)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we init ray, because we need to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Make a default environment, and train a PPO agent for one iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of the documentation, directly\n",
    "# see https://docs.ray.io/en/latest/rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.html\n",
    "\n",
    "# Construct a generic config object, specifying values within different\n",
    "# sub-categories, e.g. \"training\".\n",
    "config = (PPOConfig().training(gamma=0.9, lr=0.01)\n",
    "          .environment(env=Grid2opEnv, env_config={})\n",
    "          .resources(num_gpus=0)\n",
    "          .env_runners(num_env_runners=0)\n",
    "          .framework(\"tf2\")\n",
    "         )\n",
    "\n",
    "# A config object can be used to construct the respective Algorithm.\n",
    "rllib_algo = config.build()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train it for one training iteration (might call `env.reset()` and  `env.step()` multiple times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(rllib_algo.train())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Train a PPO agent using 2 \"runners\" to make the rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://docs.ray.io/en/latest/rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.html\n",
    "\n",
    "# use multiple use multiple runners\n",
    "config2 = (PPOConfig().training(gamma=0.9, lr=0.01)\n",
    "           .environment(env=Grid2opEnv, env_config={})\n",
    "           .resources(num_gpus=0)\n",
    "           .env_runners(num_env_runners=2, num_envs_per_env_runner=1, num_cpus_per_env_runner=1)\n",
    "           .framework(\"tf2\")\n",
    "          )\n",
    "\n",
    "# A config object can be used to construct the respective Algorithm.\n",
    "rllib_algo2 = config2.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train it for one training iteration (might call `env.reset()` and  `env.step()` multiple times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rllib_algo2.train())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Use non default parameters to make the l2rpn environment\n",
    "\n",
    "In this first example, we will train a policy using the \"box\" action space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://docs.ray.io/en/latest/rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.html\n",
    "\n",
    "# Use a \"Box\" action space (mainly to use redispatching, curtailment and storage units)\n",
    "env_config = {\"env_name\": \"l2rpn_idf_2023\",\n",
    "              \"env_is_test\": True,\n",
    "              \"act_type\": \"box\",\n",
    "             }\n",
    "config3 = (PPOConfig().training(gamma=0.9, lr=0.01)\n",
    "           .environment(env=Grid2opEnv, env_config=env_config)\n",
    "           .resources(num_gpus=0)\n",
    "           .env_runners(num_env_runners=2, num_envs_per_env_runner=1, num_cpus_per_env_runner=1)\n",
    "           .framework(\"tf2\")\n",
    "          )\n",
    "\n",
    "# A config object can be used to construct the respective Algorithm.\n",
    "rllib_algo3 = config3.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train it for one training iteration (might call `env.reset()` and  `env.step()` multiple times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rllib_algo3.train())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now a policy using the \"multi discrete\" action space: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://docs.ray.io/en/latest/rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.html\n",
    "\n",
    "# Use a \"Box\" action space (mainly to use redispatching, curtailment and storage units)\n",
    "env_config4 = {\"env_name\": \"l2rpn_idf_2023\",\n",
    "               \"env_is_test\": True,\n",
    "               \"act_type\": \"multi_discrete\",\n",
    "               }\n",
    "config4 = (PPOConfig().training(gamma=0.9, lr=0.01)\n",
    "           .environment(env=Grid2opEnv, env_config=env_config4)\n",
    "           .resources(num_gpus=0)\n",
    "           .env_runners(num_env_runners=2, num_envs_per_env_runner=1, num_cpus_per_env_runner=1)\n",
    "           .framework(\"tf2\")\n",
    "          )\n",
    "\n",
    "# A config object can be used to construct the respective Algorithm.\n",
    "rllib_algo4 = config4.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train it for one training iteration (might call `env.reset()` and  `env.step()` multiple times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rllib_algo4.train())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Customize the policy (number of layers, size of layers etc.)\n",
    "\n",
    "This notebook does not aim at covering all possibilities offered by ray / rllib. For that you need to refer to the ray / rllib documentation.\n",
    "\n",
    "We will simply show how to change the size of the neural network used as a policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://docs.ray.io/en/latest/rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.html\n",
    "\n",
    "# Use a \"Box\" action space (mainly to use redispatching, curtailment and storage units)\n",
    "config5 = (PPOConfig().training(gamma=0.9, lr=0.01)\n",
    "           .environment(env=Grid2opEnv, env_config={})\n",
    "           .resources(num_gpus=0)\n",
    "           .env_runners(num_env_runners=2, num_envs_per_env_runner=1, num_cpus_per_env_runner=1)\n",
    "           .framework(\"tf2\")\n",
    "           .rl_module(\n",
    "             model_config_dict={\"fcnet_hiddens\": [32, 32, 32]},  # 3 layers (fully connected) of 32 units each\n",
    "           )\n",
    "          )\n",
    "\n",
    "# A config object can be used to construct the respective Algorithm.\n",
    "rllib_algo5 = config5.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train it for one training iteration (might call `env.reset()` and  `env.step()` multiple times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rllib_algo5.train())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
